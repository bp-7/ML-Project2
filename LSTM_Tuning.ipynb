{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM_Tuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"NsVqqLsCfdSJ","colab_type":"text"},"source":["# Hyper-parameter tuning for GloVe embedding and LSTM network"]},{"cell_type":"markdown","metadata":{"id":"-udvPq1iJ11F","colab_type":"text"},"source":["### Set up environnent, access to google drive, import librairies\n"]},{"cell_type":"code","metadata":{"id":"Z82p8EF0_c6X","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l1yEtNy__h1G","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vLKe7zaw_ieM","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","import os, sys\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('Problem with GPU device')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QPIAX7zQIXla","colab_type":"text"},"source":["### Import librairies"]},{"cell_type":"code","metadata":{"id":"WADKxhWk_kts","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","sys.path.insert(0, os.path.abspath('/content/drive/My Drive/ML_Project_2/'))\n","import sys, os\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from helpers import *\n","from models_LSTM import *\n","from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.python.keras.layers.embeddings import Embedding\n","from sklearn import model_selection\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.sequence import pad_sequences\n","import re\n","\n","tf.compat.v1.get_default_graph"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNeWckZ1ALyG","colab_type":"code","colab":{}},"source":["# Define path to Google Drive \n","path_g = \"/content/drive/My Drive/ML_Project_2/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"shSSV_DZIiyH","colab_type":"text"},"source":["### Define useful functions\n"]},{"cell_type":"code","metadata":{"id":"a_QuDCsC_wBY","colab_type":"code","colab":{}},"source":["def not_processing(data_train, data_test):\n","    \"\"\"\n","    Apply negation processing \n","\n","    :param data_train: Training dataset\n","    :param data_test: Test dataset\n","    :return: Training and test dataset with negation processing\n","    \"\"\"\n","    for idx, tweet in enumerate(data_train):\n","        tweet = ' '.join(tweet)\n","        data_train[idx] = re.sub(r\"\\w+n't\\s?\", 'not ', tweet)\n","\n","    for idx, tweet in enumerate(data_test):\n","        tweet = ' '.join(tweet)\n","        data_test[idx] = re.sub(r\"\\w+n't\\s?\", 'not ', tweet)\n","\n","    to_not = ['havent', 'doesnt', 'cant', 'dont', 'shouldnt', 'arent', 'couldnt', \"didnt\", \"hadnt\", \"mightnt\",\n","              \"mustnt\", \"neednt\", \"wasnt\", \"wont\", \"wouldnt\", 'neednt', 'isnt', 'werent']\n","\n","    for word in to_not:\n","        data_train = [re.sub(r'\\b' + word + r'\\b', 'not', tweet) for tweet in data_train]\n","        data_test = [re.sub(r'\\b' + word + r'\\b', 'not', tweet) for tweet in data_test]\n","\n","    return data_train, data_test\n","\n","\n","def tokenize(data_train, data_test, len_max_tweet, n_dim):\n","    \"\"\"\n","    Tokenize tweets and load embedding matrix\n","    \n","    :param data_train: Training dataset\n","    :param data_test: Test dataset\n","    :param len_max_tweet: Maximum length of the tweets in the datasets\n","    :param n_dim: Embedding dimension\n","    :return: Tokenized training and test dataset, size of the dataset vocabulary and embedding matrix\n","    \"\"\"\n","    # Create a tokenizer instance\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1e6)\n","\n","    # Fit the tokenizer on the training set\n","    tokenizer.fit_on_texts(data_train)\n","\n","    # Tokenize data\n","    data_train = tokenizer.texts_to_sequences(data_train)\n","    data_test = tokenizer.texts_to_sequences(data_test)\n","\n","    # Compute vocabulary size\n","    vocab_size = len(tokenizer.word_index) + 1\n","\n","    embeddings_dictionary = dict()\n","    glove_file = open(path_g + 'glove/glove.twitter.27B.' + str(n_dim) + 'd.txt', encoding=\"utf8\")\n","\n","    # Extract glove matrix \n","    for line in glove_file:\n","        records = line.split()\n","        word = records[0]\n","        vector_dimensions = np.asarray(records[1:], dtype='float32')\n","        embeddings_dictionary[word] = vector_dimensions\n","    glove_file.close()\n","\n","    embedding_matrix = np.zeros((vocab_size, n_dim))\n","    for word, index in tokenizer.word_index.items():\n","        embedding_vector = embeddings_dictionary.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","\n","    return data_train, data_test, vocab_size, embedding_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOtqswQV3ag2","colab_type":"text"},"source":["### Load data"]},{"cell_type":"code","metadata":{"id":"aR7zfrPW_ykh","colab_type":"code","colab":{}},"source":["print(\"Loading Data ...\")\n","# Load full dataset or not: 'f' or 'nf'\n","full='nf'\n","processed=False\n","\n","if processed:\n","    data_train = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_train_pr_' + full + '_sl5' + '.npy', allow_pickle=True)\n","    data_test = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_test_pr_sl5' + '.npy', allow_pickle=True)\n","    labels = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/labels_train_'+ full +'_sl5.npy')\n","    dataset_type = 'processed'\n","\n","else:\n","    data_train, labels, data_test = get_raw_data(path_g, full)\n","    dataset_type = 'raw'\n","\n","# If labels are -1 instead of 0\n","labels = np.where(labels == -1, 0, labels)\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pmBU18ye1aCS","colab_type":"code","colab":{}},"source":["perm = np.random.permutation(data_train.shape[0])\n","data_train = data_train[perm]\n","labels = labels[perm]\n","\n","# To train without the full set\n","n_train = 200\n","\n","if n_train > 0:\n","    data_train = data_train[:n_train]\n","    labels = labels[:n_train]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IS1xOeTE1hDF","colab_type":"code","colab":{}},"source":["print(\"Computing maximal length of tweets\", flush=True)\n","\n","# Max length of tweet (after removed not in vocab words)\n","len_max_tweet = np.max([len(tweet) for tweet in data_train])\n","len_max_tweet = np.max((len_max_tweet, np.max([len(tweet) for tweet in data_test])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2Y8iXZxJ_020","colab_type":"code","colab":{}},"source":["print(\"Start to convert negative words\", flush=True)\n","\n","# Negation processing\n","data_train_not, data_test_not = not_processing(data_train, data_test)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IfxmT7494AN8","colab_type":"text"},"source":["### Define callback for the hyperparameter tuning"]},{"cell_type":"code","metadata":{"id":"jP5bNiZHKX2T","colab_type":"code","colab":{}},"source":["from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n","\n","class GetAcc(Callback):\n","    def __init__(self, data_test, labels_test, acc_array, kfold_idx):\n","        self.data_test = data_test\n","        self.labels_test = labels_test\n","        self.acc = acc_array\n","        self.kfold_idx = kfold_idx\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        loss, accuracy = self.model.evaluate(self.data_test, self.labels_test, verbose=0)\n","        self.acc[self.kfold_idx, epoch] = accuracy\n","        #print('kfold_idx:', self.kfold_idx, ' Epochs #', epoch)\n","        #print('\\nTesting loss: {}, acc: {}\\n'.format(loss, accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hhKwc5n_HKJo","colab_type":"text"},"source":["### Hyperparameter tuning"]},{"cell_type":"code","metadata":{"id":"HHtucwja_79N","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","\n","results = pd.DataFrame(columns=['ed', 'nf', 'lr', 'bs', 'ep', 'accuracy', 'mean_accuracy', 'std_accuracy'])\n","\n","# Number of k-folds\n","n_splits = 5\n","\n","# Define parameters to tune \n","epochs = 9\n","filters_arr = [100, 200, 300, 400]\n","n_dims = [50, 100, 200]\n","lr = 1e-3;\n","batch_sizes = [32, 64, 128, 256]\n","\n","for n_dim in n_dims:\n","  print('ndim = ', n_dim)\n","\n","  data_test = data_test_not.copy()\n","  data_train, y_train = data_train_not.copy(), labels.copy()\n","\n","  data_train, data_test, vocab_size, embedding_matrix = tokenize(data_train, data_test, len_max_tweet, n_dim)\n","\n","  data_train = pad_sequences(data_train, padding='post', maxlen=len_max_tweet)\n","  data_test = pad_sequences(data_test, padding='post', maxlen=len_max_tweet)\n","\n","  for filters in filters_arr:\n","      print('filter = ', filters)\n","\n","      for batch_size in batch_sizes:\n","        print('batch_size = ', batch_size)\n","\n","        acc_array = np.zeros((n_splits, epochs))\n","        k_i = 0\n","\n","        for train_index, test_index in KFold(n_splits).split(data_train):\n","            x_train, x_test = data_train[train_index], data_train[test_index]\n","            y_train_, y_test = y_train[train_index], y_train[test_index]    \n","            \n","            model = build_model_lstm_emb(filters, vocab_size, n_dim, embedding_matrix, len_max_tweet, lr)\n","            \n","            if (k_i == 0):\n","              model.summary()\n","\n","            print('k_fold:', k_i)\n","\n","            model.fit(x_train, y_train_,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    validation_data=(x_test, y_test),\n","                    callbacks=[GetAcc(x_test, y_test, acc_array, k_i)])\n","            k_i += 1\n","\n","        for n in range(epochs):\n","            results = results.append({'ed': n_dim, 'nf': filters, 'lr': lr, 'bs': batch_size, 'ep': n+1,\n","                                        'accuracy': acc_array[:, n], 'mean_accuracy': np.mean(acc_array[:, n]), \n","                                        'std_accuracy': np.std(acc_array[:, n])}, ignore_index=True)\n","\n","imax = results['mean_accuracy'].idxmax()\n","print(\"Best: %f using \\n %s\" % (results['mean_accuracy'].iloc[imax], results.iloc[imax]))\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MWBFPRrUu6BE","colab_type":"text"},"source":["### Save best combination of parameters"]},{"cell_type":"code","metadata":{"id":"bIHw4i82WLGZ","colab_type":"code","colab":{}},"source":["imax = np.argmax(results['mean_accuracy'].values)\n","print(\"Best: %f using \\n %s\" % (results['mean_accuracy'].iloc[imax], results.iloc[imax]))\n","\n","x = results['accuracy'].iloc[imax]\n","\n","np.save(path_g + 'results_LSTM_' + dataset_type, x)"],"execution_count":0,"outputs":[]}]}