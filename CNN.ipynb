{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZnibz2jcgf8",
        "colab_type": "text"
      },
      "source": [
        "### Set up environnent, access to google drive, import librairies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPBtCaDbX8W",
        "colab_type": "code",
        "outputId": "de9304e5-b1bb-4446-ba81-186e874a10cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTY4SxCVVlgx",
        "colab_type": "code",
        "outputId": "86432979-b6c7-4e0c-f5fa-bf25c4236171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgjCgrKKe2Z",
        "colab_type": "code",
        "outputId": "87499e27-f436-4d9e-9987-fd0579139324",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "# Need for last version of keras for compatibility with gensim\n",
        "pip install keras --upgrade"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 29.3MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 34.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 40.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 44.9MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 37.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 40.3MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 27.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 29.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 31.6MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 30.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 30.2MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 30.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 30.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /tensorflow-2.1.0/python3.6 (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /tensorflow-2.1.0/python3.6 (from keras) (1.1.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK35ZN3cop4T",
        "colab_type": "code",
        "outputId": "6324499c-ba5f-4edc-e3d0-dee4fe41c0e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os, sys\n",
        "sys.path.insert(0, os.path.abspath('/content/drive/My Drive/ML_Project_2/'))\n",
        "from models_CNN import *\n",
        "from helpers import *\n",
        "\n",
        "path_g = '/content/drive/My Drive/ML_Project_2/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KMSpQZiqUqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and set seed for reproductibility \n",
        "import numpy as np\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "import time\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "import gensim\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "import keras\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYurLfyXFjOT",
        "colab_type": "text"
      },
      "source": [
        "###Load the data, processed or not and shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB_rMVxsVjNz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load full dataset or not: 'f' or 'nf'\n",
        "full='f'\n",
        "processed=False\n",
        "\n",
        "if processed:\n",
        "    text_data = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_train_pr_' + full + '_sl5' + '.npy', allow_pickle=True)\n",
        "    text_data_test = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_test_pr_sl5' + '.npy', allow_pickle=True)\n",
        "    labels = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/labels_train_'+ full +'_sl5.npy')\n",
        "    dataset_type = 'processed'\n",
        "\n",
        "else:\n",
        "    text_data, labels, text_data_test = get_raw_data(path_g, full)\n",
        "    dataset_type = 'raw'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVNyqBHkVkaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm = np.random.permutation(text_data.shape[0])\n",
        "text_data = text_data[perm]\n",
        "labels = labels[perm]\n",
        "\n",
        "# If we don't want to take full dataset\n",
        "n_train = -1\n",
        "\n",
        "if n_train > 0:\n",
        "    text_data = text_data[:n_train]\n",
        "    labels = labels[:n_train]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdGiB1mCdn1r",
        "colab_type": "text"
      },
      "source": [
        "### Train or load gensim Word2Vec models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUQFw-5_qaMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_w2v = True\n",
        "save_w2v = True\n",
        "\n",
        "# Define gensim model\n",
        "# Size of embedding\n",
        "size_w2v = 400\n",
        "# number of iteration\n",
        "iter_w2v = 5\n",
        "# window size\n",
        "window_w2v = 5\n",
        "# min count\n",
        "min_count = 6\n",
        "\n",
        "if train_w2v:\n",
        "  # Take all dataset to train gensim word2vec\n",
        "  text_data_tot = np.concatenate((text_data, text_data_test), axis=0)\n",
        "\n",
        "  t1 = time.time()\n",
        "\n",
        "\n",
        "  # Name to save the model afterwards\n",
        "\n",
        "  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n",
        "  name_w2v = 'w2v_model'\n",
        "\n",
        "  # Train gensim model (skipgram)\n",
        "  model_gs = gensim.models.Word2Vec(text_data_tot, size=size_w2v, window=window_w2v, sg=1,\\\n",
        "                                    min_count=min_count, iter=iter_w2v)\n",
        "  # Convert to word_vector and save\n",
        "  word_vector = model_gs.wv\n",
        "  print(\"Total time to train gensim\", time.time() - t1, \"s\", flush=True)\n",
        "  \n",
        "  if save_w2v:\n",
        "    word_vector.save(path_w2v+name_w2v)\n",
        "\n",
        "\n",
        "else:\n",
        "  # Specify path and name of gensim file\n",
        "  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n",
        "  name_w2v = 'w2v_model'\n",
        "  word_vector = gensim.models.KeyedVectors.load(path_w2v + name_w2v)  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quh-0VySFvMn",
        "colab_type": "text"
      },
      "source": [
        "###Insert our Word2Vec model into keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmZsn5rMU1in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert gensim word_vector in keras embedding\n",
        "# Choose or not to continue embedding training during network training\n",
        "train_emb = True\n",
        "k_emb = word_vector.get_keras_embedding(train_embeddings=train_emb)\n",
        "size_emb = k_emb.output_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzWXWE7boWS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert text to numerical data according to gensim (now keras embedding) vocabulary \n",
        "vocabulary = {word: vector.index for word, vector in word_vector.vocab.items()}\n",
        "tk = Tokenizer(num_words=len(vocabulary))\n",
        "tk.word_index = vocabulary\n",
        "num_data = np.asarray((pad_sequences(tk.texts_to_sequences(text_data), padding='post')))\n",
        "num_data_test = np.asarray((pad_sequences(tk.texts_to_sequences(text_data_test), \n",
        "                                          maxlen=num_data.shape[1],padding='post')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42-1V_5GDAm",
        "colab_type": "text"
      },
      "source": [
        "###Define our convolutional network, with parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynARJYguqLRq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "filters, kernel_size, batch_size = 300, 5, 150\n",
        "epochs = 2\n",
        "hidden_dims =  250\n",
        "learning_rate =  0.001\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "model = build_model_emb(k_emb, filters, kernel_size, hidden_dims, num_data.shape[1], \n",
        "                        size_emb, learning_rate, dropout=dropout)\n",
        "model.summary()\n",
        "\n",
        "x_train, y_train = num_data, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbudiNlcplKX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjY1Bq8GWJ0",
        "colab_type": "text"
      },
      "source": [
        "### Generate a submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BQ34BNHpoCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on test set\n",
        "y_pred = np.ndarray.flatten(model.predict_classes(num_data_test, batch_size=batch_size))\n",
        "\n",
        "# Replace for submission\n",
        "y_pred = np.where(y_pred == 0, -1, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEAGzYG6boaA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate submission\n",
        "path_csv = '/content/drive/My Drive/ML_Project_2/'\n",
        "csv_name ='sub_real'\n",
        "\n",
        "create_csv_submission(y_pred, path_csv + csv_name + '.csv')\n",
        "print(\"Output name:\", csv_name + '.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}