{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZnibz2jcgf8",
        "colab_type": "text"
      },
      "source": [
        "### Set up environnent, access to google drive, import librairies\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPBtCaDbX8W",
        "colab_type": "code",
        "outputId": "a69c86f6-6879-4791-b775-2b0bd2a1e70f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTY4SxCVVlgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWgjCgrKKe2Z",
        "colab_type": "code",
        "outputId": "cc88c5ae-548d-4644-d5f6-c916cd1cce98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Need for last version of keras for compatibility with gensim\n",
        "pip install keras --upgrade"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /tensorflow-2.1.0/python3.6 (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /tensorflow-2.1.0/python3.6 (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras) (2.10.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JK35ZN3cop4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "sys.path.insert(0, os.path.abspath('/content/drive/My Drive/ML_Project_2/'))\n",
        "from models_CNN import *\n",
        "from helpers import *\n",
        "\n",
        "path_g = '/content/drive/My Drive/ML_Project_2/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4KMSpQZiqUqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import and set seed for reproductibility \n",
        "import numpy as np\n",
        "seed = 7\n",
        "np.random.seed(seed)\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "import time\n",
        "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
        "import gensim\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "import keras\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYurLfyXFjOT",
        "colab_type": "text"
      },
      "source": [
        "###Load the data, processed or not and shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB_rMVxsVjNz",
        "colab_type": "code",
        "outputId": "c16ad6df-9c74-4738-cb12-e900665203fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Load full dataset or not: 'f' or 'nf'\n",
        "full='f'\n",
        "processed=False\n",
        "\n",
        "if processed:\n",
        "    text_data = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_train_pr_' + full + '_sl5' + '.npy', allow_pickle=True)\n",
        "    text_data_test = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_test_pr_sl5' + '.npy', allow_pickle=True)\n",
        "    labels = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/labels_train_'+ full +'_sl5.npy')\n",
        "    dataset_type = 'processed'\n",
        "\n",
        "else:\n",
        "    text_data, labels, text_data_test = get_raw_data(path_g, full)\n",
        "    dataset_type = 'raw'\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1142838, 2)\n",
            "(1127644, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVNyqBHkVkaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "perm = np.random.permutation(text_data.shape[0])\n",
        "text_data = text_data[perm]\n",
        "labels = labels[perm]\n",
        "\n",
        "# If we don't want to take full dataset\n",
        "n_train = -1\n",
        "\n",
        "if n_train > 0:\n",
        "    text_data = text_data[:n_train]\n",
        "    labels = labels[:n_train]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdGiB1mCdn1r",
        "colab_type": "text"
      },
      "source": [
        "### Train or load gensim Word2Vec models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUQFw-5_qaMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "25b11131-0e5f-4e79-e612-8ef454ecc8ee"
      },
      "source": [
        "train_w2v = False\n",
        "save_w2v = True\n",
        "\n",
        "# Define gensim model\n",
        "# Size of embedding\n",
        "size_w2v = 400\n",
        "# number of iteration\n",
        "iter_w2v = 5\n",
        "# window size\n",
        "window_w2v = 5\n",
        "# min count\n",
        "min_count = 6\n",
        "\n",
        "if train_w2v:\n",
        "  # Take all dataset to train gensim word2vec\n",
        "  text_data_tot = np.concatenate((text_data, text_data_test), axis=0)\n",
        "\n",
        "  t1 = time.time()\n",
        "\n",
        "\n",
        "  # Name to save the model afterwards\n",
        "\n",
        "  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n",
        "  name_w2v = 'w2v_model_best'\n",
        "\n",
        "  # Train gensim model (skipgram)\n",
        "  model_gs = gensim.models.Word2Vec(text_data_tot, size=size_w2v, window=window_w2v, sg=1,\\\n",
        "                                    min_count=min_count, iter=iter_w2v)\n",
        "  # Convert to word_vector and save\n",
        "  word_vector = model_gs.wv\n",
        "  print(\"Total time to train gensim\", time.time() - t1, \"s\", flush=True)\n",
        "  \n",
        "  if save_w2v:\n",
        "    word_vector.save(path_w2v+name_w2v)\n",
        "\n",
        "\n",
        "else:\n",
        "  # Specify path and name of gensim file\n",
        "  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n",
        "  name_w2v = 'w2v_model_best'\n",
        "  word_vector = gensim.models.KeyedVectors.load(path_w2v + name_w2v)  \n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quh-0VySFvMn",
        "colab_type": "text"
      },
      "source": [
        "###Insert our Word2Vec model into keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zmZsn5rMU1in",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert gensim word_vector in keras embedding\n",
        "# Choose or not to continue embedding training during network training\n",
        "train_emb = True\n",
        "k_emb = word_vector.get_keras_embedding(train_embeddings=train_emb)\n",
        "size_emb = k_emb.output_dim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzWXWE7boWS1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert text to numerical data according to gensim (now keras embedding) vocabulary \n",
        "vocabulary = {word: vector.index for word, vector in word_vector.vocab.items()}\n",
        "tk = Tokenizer(num_words=len(vocabulary))\n",
        "tk.word_index = vocabulary\n",
        "num_data = np.asarray((pad_sequences(tk.texts_to_sequences(text_data), padding='post')))\n",
        "num_data_test = np.asarray((pad_sequences(tk.texts_to_sequences(text_data_test), \n",
        "                                          maxlen=num_data.shape[1],padding='post')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T42-1V_5GDAm",
        "colab_type": "text"
      },
      "source": [
        "###Define our convolutional network, with parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynARJYguqLRq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "f8b670c2-f07b-4471-d859-c1c7ec358f4f"
      },
      "source": [
        "filters, kernel_size, batch_size = 300, 5, 150\n",
        "epochs = 2\n",
        "hidden_dims =  250\n",
        "learning_rate =  0.001\n",
        "dropout = 0.2\n",
        "\n",
        "\n",
        "model = build_model_emb(k_emb, filters, kernel_size, hidden_dims, num_data.shape[1], \n",
        "                        size_emb, learning_rate, dropout=dropout)\n",
        "model.summary()\n",
        "\n",
        "x_train, y_train = num_data, labels"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (None, None, 400)         28794800  \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, None, 300)         600300    \n",
            "_________________________________________________________________\n",
            "global_max_pooling1d_2 (Glob (None, 300)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 250)               75250     \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 250)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 251       \n",
            "=================================================================\n",
            "Total params: 29,470,601\n",
            "Trainable params: 29,470,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbudiNlcplKX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "c7002529-5b65-4a48-ccac-ac394c22f0c4"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "2270482/2270482 [==============================] - 441s 194us/step - loss: 0.3477 - accuracy: 0.8429\n",
            "Epoch 2/2\n",
            "2270482/2270482 [==============================] - 439s 193us/step - loss: 0.2949 - accuracy: 0.8716\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.callbacks.History at 0x7f8d93ec79b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwjY1Bq8GWJ0",
        "colab_type": "text"
      },
      "source": [
        "### Generate a submission file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BQ34BNHpoCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Predict on test set\n",
        "y_pred = np.ndarray.flatten(model.predict_classes(num_data_test, batch_size=batch_size))\n",
        "\n",
        "# Replace for submission\n",
        "y_pred = np.where(y_pred == 0, -1, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEAGzYG6boaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0af9c8b1-ec3d-4e9d-baa3-9367353d8644"
      },
      "source": [
        "# Generate submission\n",
        "path_csv = '/content/drive/My Drive/ML_Project_2/'\n",
        "csv_name ='sub_best'\n",
        "\n",
        "create_csv_submission(y_pred, path_csv + csv_name + '.csv')\n",
        "print(\"Output name:\", csv_name + '.csv')"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output name: sub_real.csv\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}