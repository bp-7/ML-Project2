{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LSTM.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"uvHQ61KMve_C","colab_type":"text"},"source":["### Set up environnent, access to google drive, import librairies"]},{"cell_type":"code","metadata":{"id":"i-ZZbcCALapa","colab_type":"code","colab":{}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ArYaLaLMLjBG","colab_type":"code","colab":{}},"source":["%tensorflow_version 2.x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"c7_6Ej_HLkb8","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('Problem with GPU device')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CRLiaBCv4Gr","colab_type":"text"},"source":["### Import librairies\n"]},{"cell_type":"code","metadata":{"id":"EdKGu86WLm7i","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import sys, os\n","sys.path.insert(0, os.path.abspath('/content/drive/My Drive/ML_Project_2/'))\n","import numpy as np\n","from helpers import *\n","import tensorflow as tf\n","from models_LSTM import *\n","from tensorflow.python.keras.layers.embeddings import Embedding\n","from keras.preprocessing.sequence import pad_sequences\n","import re\n","\n","tf.compat.v1.get_default_graph\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FVU0dqRYaeDR","colab_type":"code","colab":{}},"source":["path_g = \"/content/drive/My Drive/ML_Project_2/\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7q6xrRwcv_Fb","colab_type":"text"},"source":["### Define useful functions\n"]},{"cell_type":"code","metadata":{"id":"FfmBhvExL2Dk","colab_type":"code","colab":{}},"source":["def not_treatment(data_train, data_val, data_test):\n","    \"\"\"\n","    Apply negation processing \n","\n","    :param data_train: Training dataset\n","    :param data_test: Test dataset\n","    :return: Training and test dataset with negation processing\n","    \"\"\"\n","    for idx, tweet in enumerate(data_train):\n","        tweet = ' '.join(tweet)\n","        data_train[idx] = re.sub(r\"\\w+n't\\s?\", 'not ', tweet)\n","\n","    for idx, tweet in enumerate(data_test):\n","        tweet = ' '.join(tweet)\n","        data_test[idx] = re.sub(r\"\\w+n't\\s?\", 'not ', tweet)\n","\n","    for idx, tweet in enumerate(data_val):\n","        tweet = ' '.join(tweet)\n","        data_val[idx] = re.sub(r\"\\w+n't\\s?\", 'not ', tweet)\n","\n","    to_not = ['havent', 'doesnt', 'cant', 'dont', 'shouldnt', 'arent', 'couldnt', \"didnt\", \"hadnt\", \"mightnt\",\n","              \"mustnt\", \"neednt\", \"wasnt\", \"wont\", \"wouldnt\", 'neednt', 'isnt', 'werent']\n","\n","    for word in to_not:\n","        data_train = [re.sub(r'\\b' + word + r'\\b', 'not', tweet) for tweet in data_train]\n","        data_test = [re.sub(r'\\b' + word + r'\\b', 'not', tweet) for tweet in data_test]\n","        data_val = [re.sub(r'\\b' + word + r'\\b', 'not', tweet) for tweet in data_val]\n","\n","    return data_train, data_val, data_test\n","\n","\n","def tokenize(data_train, data_val, data_test, len_max_tweet, n_dim):\n","    \"\"\"\n","    Tokenize tweets and load embedding matrix\n","    \n","    :param data_train: Training dataset\n","    :param data_test: Test dataset\n","    :param len_max_tweet: Maximum length of the tweets in the datasets\n","    :param n_dim: Embedding dimension\n","    :return: Tokenized training and test dataset, size of the dataset vocabulary and embedding matrix\n","    \"\"\"\n","\n","    # Create a tokenizer instance\n","    tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=1e6)\n","\n","    # Fit the tokenizer on the training set\n","    tokenizer.fit_on_texts(data_train)\n","\n","    # Tokenize data\n","    data_train = tokenizer.texts_to_sequences(data_train)\n","    data_test = tokenizer.texts_to_sequences(data_test)\n","    data_val = tokenizer.texts_to_sequences(data_val)\n","\n","    # Compute vocabulary size\n","    vocab_size = len(tokenizer.word_index) + 1\n","\n","    embeddings_dictionary = dict()\n","    glove_file = open(path_g + 'glove/glove.twitter.27B.' + str(n_dim) + 'd.txt', encoding=\"utf8\")  \n","\n","    for line in glove_file:\n","        records = line.split()\n","        word = records[0]\n","        vector_dimensions = np.asarray(records[1:], dtype='float32')\n","        embeddings_dictionary[word] = vector_dimensions\n","    glove_file.close()\n","\n","    embedding_matrix = np.zeros((vocab_size, n_dim))\n","    for word, index in tokenizer.word_index.items():\n","        embedding_vector = embeddings_dictionary.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[index] = embedding_vector\n","\n","    embedding_layer = Embedding(vocab_size, n_dim, weights=[embedding_matrix], input_length=len_max_tweet,\n","                                trainable=False)\n","\n","    return data_train, data_val, data_test, embedding_layer"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GeUljUZWxs22","colab_type":"text"},"source":["### Load data"]},{"cell_type":"code","metadata":{"id":"0on2LJEmNnwS","colab_type":"code","colab":{}},"source":["print(\"Loading Data ...\")\n","# Load full dataset or not: 'f' or 'nf'\n","full='f'\n","processed=False\n","\n","if processed:\n","    data_train = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_train_pr_' + full + '_sl5' + '.npy', allow_pickle=True)\n","    data_test = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_test_pr_sl5' + '.npy', allow_pickle=True)\n","    labels = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/labels_train_'+ full +'_sl5.npy')\n","    dataset_type = 'processed'\n","\n","else:\n","    data_train, labels, data_test = get_raw_data(path_g, full)\n","    dataset_type = 'raw'\n","\n","# If labels are -1 instead of 0\n","labels = np.where(labels == -1, 0, labels)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sCUF5aIPNFru","colab_type":"code","colab":{}},"source":["perm = np.random.permutation(data_train.shape[0])\n","data_train = data_train[perm]\n","labels = labels[perm]\n","\n","# To train without the full set\n","n_train = -1\n","\n","if n_train > 0:\n","    data_train = data_train[:n_train]\n","    labels = labels[:n_train]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkHrETAmNG8l","colab_type":"code","colab":{}},"source":["print(\"Computing maximal length of tweets\", flush=True)\n","\n","# Max length of tweet (after removed not in vocab words)\n","len_max_tweet = np.max([len(tweet) for tweet in data_train])\n","len_max_tweet = np.max((len_max_tweet, np.max([len(tweet) for tweet in data_test])))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NQf6uh1NNT3J","colab_type":"code","colab":{}},"source":["print(\"Start to convert negative words\", flush=True)\n","\n","# Negation processing\n","data_train, _, data_test = not_treatment(data_train, data_test.copy(), data_test)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AlavJvF_fLSf","colab_type":"code","colab":{}},"source":["print(\"Start to tokenize\", flush=True)\n","\n","n_dim = 200\n","data_train, _, data_test, embedding_layer = tokenize(data_train, data_test.copy(), data_test, len_max_tweet, n_dim)\n","\n","data_train = pad_sequences(data_train, padding='post', maxlen=len_max_tweet)\n","data_test = pad_sequences(data_test, padding='post', maxlen=len_max_tweet)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-U8D3Pn4LetU","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","\n","# Define neural network parameters\n","filters_lstm, batch_size = 400, 64\n","epochs = 5\n","\n","model = build_model_lstm_emb_(filters_lstm, embedding_layer)\n","\n","model.summary()\n","\n","model.fit(x=data_train, y=labels, epochs=epochs, verbose=1, batch_size=batch_size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E0Yp3EZe6pvy","colab_type":"text"},"source":["### Make prediction and generate a submission file"]},{"cell_type":"code","metadata":{"id":"VVmypSRedd-0","colab_type":"code","colab":{}},"source":["y_pred = np.ndarray.flatten(model.predict_classes(data_test, batch_size=batch_size))\n","\n","# Replace for submission\n","y_pred = np.where(y_pred == 0, -1, y_pred)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FiEbeWg2CBff","colab_type":"code","colab":{}},"source":["csv_name = path_g + 'sub_LSTM'\n","\n","create_csv_submission(y_pred, csv_name + '.csv')\n","print(\"Output name:\", csv_name)"],"execution_count":0,"outputs":[]}]}