{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN_Tuning.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"aZnibz2jcgf8","colab_type":"text"},"source":["### Set up environnent, access to google drive, import librairies\n"]},{"cell_type":"code","metadata":{"id":"KDPBtCaDbX8W","colab_type":"code","outputId":"ea025da9-d9ce-4251-cf72-13a6ef25d63c","executionInfo":{"status":"ok","timestamp":1576691755127,"user_tz":-60,"elapsed":26813,"user":{"displayName":"Damien Ronssin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2pg7KXGd8-sYDf1KMyNC4OcJPQBGs0MgwBknp7w=s64","userId":"15938738448638941049"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mTY4SxCVVlgx","colab_type":"code","outputId":"ebf3e4ea-e3e7-40c9-dba2-289ee89d12d3","executionInfo":{"status":"ok","timestamp":1576691761808,"user_tz":-60,"elapsed":979,"user":{"displayName":"Damien Ronssin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2pg7KXGd8-sYDf1KMyNC4OcJPQBGs0MgwBknp7w=s64","userId":"15938738448638941049"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%tensorflow_version 2.x"],"execution_count":2,"outputs":[{"output_type":"stream","text":["TensorFlow 2.x selected.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWgjCgrKKe2Z","colab_type":"code","outputId":"9651f1c2-f7ee-4bf5-faa9-1b8a97020374","executionInfo":{"status":"ok","timestamp":1576691767768,"user_tz":-60,"elapsed":5939,"user":{"displayName":"Damien Ronssin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2pg7KXGd8-sYDf1KMyNC4OcJPQBGs0MgwBknp7w=s64","userId":"15938738448638941049"}},"colab":{"base_uri":"https://localhost:8080/","height":298}},"source":["pip install keras --upgrade"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting keras\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n","\r\u001b[K     |▉                               | 10kB 21.2MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 6.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 9.4MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 6.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 7.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 12.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 9.8MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras) (1.17.4)\n","Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras) (2.10.0)\n","Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /tensorflow-2.1.0/python3.6 (from keras) (1.1.0)\n","Requirement already satisfied, skipping upgrade: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras) (1.13.0)\n","Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /tensorflow-2.1.0/python3.6 (from keras) (1.0.8)\n","Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n","Installing collected packages: keras\n","  Found existing installation: Keras 2.2.5\n","    Uninstalling Keras-2.2.5:\n","      Successfully uninstalled Keras-2.2.5\n","Successfully installed keras-2.3.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"JK35ZN3cop4T","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"a67b4761-7f63-4912-839d-73beb4e395e2","executionInfo":{"status":"ok","timestamp":1576691786541,"user_tz":-60,"elapsed":7873,"user":{"displayName":"Damien Ronssin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2pg7KXGd8-sYDf1KMyNC4OcJPQBGs0MgwBknp7w=s64","userId":"15938738448638941049"}}},"source":["import os, sys\n","sys.path.insert(0, os.path.abspath('/content/drive/My Drive/ML_Project_2/'))\n","from models import *\n","from helpers import *\n","\n","path_g = '/content/drive/My Drive/ML_Project_2/'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"4KMSpQZiqUqq","colab_type":"code","colab":{}},"source":["import numpy as np\n","import time\n","from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n","import gensim\n","from keras.callbacks import ModelCheckpoint, LearningRateScheduler, Callback\n","from keras.models import load_model\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","import tensorflow as tf\n","import keras\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vB_rMVxsVjNz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"f04cf205-d08a-4581-cbbf-3bca4fb39372","executionInfo":{"status":"ok","timestamp":1576691821149,"user_tz":-60,"elapsed":34596,"user":{"displayName":"Damien Ronssin","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mC2pg7KXGd8-sYDf1KMyNC4OcJPQBGs0MgwBknp7w=s64","userId":"15938738448638941049"}}},"source":["# Load full dataset or not: 'f' or 'nf'\n","full='f'\n","processed=False\n","\n","if processed:\n","    text_data = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_train_pr_' + full + '_sl5' + '.npy', allow_pickle=True)\n","    text_data_test = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/data_test_pr_sl5' + '.npy', allow_pickle=True)\n","    labels = np.load('/content/drive/My Drive/ML_Project_2/Processed_Data/labels_train_'+ full +'_sl5.npy')\n","    dataset_type = 'processed'\n","\n","else:\n","    text_data, labels, text_data_test = get_raw_data(path_g, full)\n","    dataset_type = 'raw'\n"],"execution_count":6,"outputs":[{"output_type":"stream","text":["(1142838, 2)\n","(1127644, 2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bVNyqBHkVkaM","colab_type":"code","colab":{}},"source":["perm = np.random.permutation(text_data.shape[0])\n","text_data = text_data[perm]\n","labels = labels[perm]\n","\n","# If we don't want to take full file\n","n_train = -1\n","\n","if n_train > 0:\n","    text_data = text_data[:n_train]\n","    labels = labels[:n_train]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sdGiB1mCdn1r","colab_type":"text"},"source":["### Train or load gensim Word2Vec model"]},{"cell_type":"code","metadata":{"id":"IUQFw-5_qaMF","colab_type":"code","colab":{}},"source":["train_w2v = True\n","if train_w2v:\n","  # Take all dataset to train gensim word2vec\n","  text_data_tot = np.concatenate((text_data, text_data_test), axis=0)\n","\n","  t1 = time.time()\n","\n","  # Define gensim model\n","  size_w2v = 400\n","  iter_w2v = 5\n","  window_w2v = 5\n","  min_count = 6\n","\n","\n","  # Name to save the model afterwards\n","  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n","  name_w2v = 'w2v_f_s' + str(size_w2v) + '_i' + str(iter_w2v) + '_w' + \\\n","              str(window_w2v) + '_mc' + str(min_count) + '_full_' + full + '_' + dataset_type\n","\n","  model_gs = gensim.models.Word2Vec(text_data_tot, size=size_w2v, window=window_w2v, sg=1,\\\n","                                    min_count=min_count, iter=iter_w2v)\n","  word_vector = model_gs.wv\n","  print(\"Total time to train gensim\", time.time() - t1, \"s\", flush=True)\n","  word_vector.save(path_w2v+name_w2v)\n","\n","\n","else:\n","  \n","  path_w2v = '/content/drive/My Drive/ML_Project_2/w2v_models/'\n","  name_w2v = 'w2v_f_s' + str(size_w2v) + '_i' + str(iter_w2v) + '_w' + \\\n","              str(window_w2v) + '_mc' + str(min_count) + '_full_' + full + '_' + dataset_type'\n","  word_vector = gensim.models.KeyedVectors.load(path_w2v + name_w2v)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zmZsn5rMU1in","colab_type":"code","colab":{}},"source":["# Convert gensim word_vector in keras embedding\n","train_emb = False\n","k_emb = word_vector.get_keras_embedding(train_embeddings=train_emb)\n","size_emb = k_emb.output_dim"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AzWXWE7boWS1","colab_type":"code","colab":{}},"source":["# Convert text to numerical data according to gensim (now keras embedding) vocabulary \n","vocabulary = {word: vector.index for word, vector in word_vector.vocab.items()}\n","tk = Tokenizer(num_words=len(vocabulary))\n","tk.word_index = vocabulary\n","num_data = np.asarray((pad_sequences(tk.texts_to_sequences(text_data), padding='post')))\n","num_data_test = np.asarray((pad_sequences(tk.texts_to_sequences(text_data_test), \n","                                          maxlen=num_data.shape[1],padding='post')))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c9a4LFCDeewH","colab_type":"text"},"source":["##Hyperparameters Tuning procedure:\n","First, the size of the embedding and the number of filters were conjointly tuned (using fixed but reasonable values for other hyper-parameters).\n","\n","Then, using the best combination for size_emb and n_filters was taken, and hidden-dims were optimized.\n","\n","And finally the batch-size (what can be seen here)"]},{"cell_type":"code","metadata":{"id":"Bds12OQUT0hv","colab_type":"code","colab":{}},"source":["class GetAcc(Callback):\n","    def __init__(self, data_test, labels_test, acc_array, kfold_idx):\n","        self.data_test = data_test\n","        self.labels_test = labels_test\n","        self.acc = acc_array\n","        self.kfold_idx = kfold_idx\n","\n","    def on_epoch_end(self, epoch, logs={}):\n","        loss, accuracy = self.model.evaluate(self.data_test, self.labels_test, verbose=0)\n","        self.acc[self.kfold_idx, epoch] = accuracy\n","        #print('kfold_idx:', self.kfold_idx, ' Epochs #', epoch)\n","        #print('\\nTesting loss: {}, acc: {}\\n'.format(loss, accuracy))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ol8jxmLelmeb","colab_type":"code","colab":{}},"source":["%%script false \n","filters, kernel_size, batch_size = 300, 5, 150\n","epochs = 8\n","hidden_dims =  250\n","learning_rate =  0.001\n","dropout = 0.2\n","\n","model = build_model_emb(k_emb, filters, kernel_size, hidden_dims, num_data.shape[1], \n","                        size_emb, learning_rate, dropout=dropout)\n","\n","results = pd.DataFrame(columns=['ed', 'train_emb', 'nf', 'ks', 'hd', 'dp', 'lr', 'bs', 'ep', 'accuracy', 'mean_accuracy', 'std_accuracy'])\n","# For loop\n","n_splits = 5\n","\n","optim_param = 'batch_size'\n","batch_size = [32, 64, 150, 250]\n","\n","for batch_size in batch_size:\n","    acc_array = np.zeros((n_splits, epochs))\n","    k_i = 0\n","    print('batch_size:', batch_size)\n","    for train_index, test_index in KFold(n_splits).split(num_data):\n","        print('k_fold:', k_i)\n","        x_train, x_test = num_data[train_index], num_data[test_index]\n","        y_train, y_test = labels[train_index], labels[test_index]    \n","        \n","        k_emb = word_vector.get_keras_embedding(train_embeddings=train_emb)\n","\n","        model = build_model_emb(k_emb, filters, kernel_size, hidden_dims, num_data.shape[1], \n","                        size_emb, learning_rate, dropout=dropout)\n","        \n","        model.fit(x_train, y_train,\n","                batch_size=batch_size,\n","                epochs=epochs,\n","                validation_data=(x_test, y_test),\n","                callbacks=[GetAcc(x_test, y_test, acc_array, k_i)])\n","        k_i += 1\n","\n","    for n in range(epochs):\n","        results = results.append({'ed': size_emb, 'train_emb': train_emb,'nf': filters, 'ks': kernel_size, 'hd': hidden_dims, \n","                                    'dp': dropout, 'lr':learning_rate, 'bs': batch_size, 'ep': n+1,\n","                                    'accuracy': acc_array[:, n], 'mean_accuracy': np.mean(acc_array[:, n]), \n","                                    'std_accuracy': np.std(acc_array[:, n])}, ignore_index=True)\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"22YCqHoKnQ9s","colab_type":"code","colab":{}},"source":["%%script false \n","path = '/content/drive/My Drive/ML_Project_2/'\n","name = 'Results_DF/res_'+ 'emb_' + str(size_emb) + '_tr_'+ str(train_emb) + '_opt_' + optim_param + '.txt'\n","print(name)\n","if not os.path.exists(path + name):\n","    results.to_csv(path + name)\n","\n","else:\n","    print(\"Path already existing !! Change name for result\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"evgXEMC6zV7O","colab_type":"code","colab":{}},"source":["%%script false \n","with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n","    print(results.loc[:, results.columns != 'accuracy'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4xY0N_HO2FnN","colab_type":"code","colab":{}},"source":["%%script false \n","imax = np.argmax(results['mean_accuracy'].values)\n","print(\"Best: %f using \\n %s\" % (results['mean_accuracy'].iloc[imax], results.iloc[imax]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QkA3Md2lfsS7","colab_type":"text"},"source":["### Cross validation with bigger number of k-fold with the different datasets: processed and raw"]},{"cell_type":"code","metadata":{"id":"ynARJYguqLRq","colab_type":"code","colab":{}},"source":["filters, kernel_size, batch_size = 300, 5, 150\n","epochs = 8\n","hidden_dims =  250\n","learning_rate =  0.001\n","dropout = 0.2\n","\n","n_splits = 10\n","\n","results_2 = pd.DataFrame(columns=['dataset', 'ed', 'train_emb', 'nf', 'ks', 'hd', 'dp', 'lr', 'bs', 'ep', 'accuracy', 'mean_accuracy', 'std_accuracy'])\n","acc_array = np.zeros((n_splits, epochs))\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LbudiNlcplKX","colab_type":"code","colab":{}},"source":["k_i = 0\n","\n","\n","for train_index, test_index in KFold(n_splits).split(num_data):\n","    print('k_fold:', k_i)\n","    x_train, x_test = num_data[train_index], num_data[test_index]\n","    y_train, y_test = labels[train_index], labels[test_index]\n","    k_emb = word_vector.get_keras_embedding(train_embeddings=train_emb)\n","    model = build_model_emb(k_emb, filters, kernel_size, hidden_dims, num_data.shape[1], \n","                        size_emb, learning_rate, dropout=dropout)\n","    model.fit(x_train, y_train,\n","                    batch_size=batch_size,\n","                    epochs=epochs,\n","                    validation_data=(x_test, y_test),\n","                    callbacks=[GetAcc(x_test, y_test, acc_array, k_i)])\n","    k_i += 1\n","\n","for n in range(epochs):\n","    results_2 = results_2.append({'dataset': dataset_type,'ed': size_emb, 'train_emb': train_emb,'nf': filters, 'ks': kernel_size, 'hd': hidden_dims, \n","                                'dp': dropout, 'lr':learning_rate, 'bs': batch_size, 'ep': n+1,\n","                                'accuracy': acc_array[:, n], 'mean_accuracy': np.mean(acc_array[:, n]), \n","                                'std_accuracy': np.std(acc_array[:, n])}, ignore_index=True)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jnjJFuwhnC-c","colab_type":"code","colab":{}},"source":["print(results_2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RhY2TFrDkLEL","colab_type":"code","colab":{}},"source":["path = '/content/drive/My Drive/ML_Project_2/'\n","name = 'Results_DF/res_'+ 'emb_' + str(size_emb) + '_tr_'+ str(train_emb) + '_' +dataset_type + '_final' + '.txt'\n","print(name)\n","if not os.path.exists(path + name):\n","    results_2.to_csv(path + name)\n","\n","else:\n","    print(\"Path already existing !! Change name for result\")"],"execution_count":0,"outputs":[]}]}